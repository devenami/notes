# Hadoop 集群配置

## 目的

该文档描述了如何从很少的节点到上千节点集群的安装和配置，如果仅仅是简单使用Hadoop，你首先应该想到在单台主机撒过安装它。

这篇文档并不包含像安全和高可用这些高级特性。

## 前置条件

* 安装Java
* 从Apache镜像下载稳定版的Hadoop

## 安装

安装Hadoop集群通常设计解压软件，你可以根据自己的操作系统，安装适当的压缩软件。

通常，集群中只有一个主机会被设计为`NameNode`名称节点作为其他主机的`ResourceManager`资源管理器，他们都是管理节点，其他服务（像Web程序代理服务和MapReduce Job History服务）都会运行在他们上面，依赖他们进行加载。

集群中其他主机的行为作为`DataNode`和`NodeManager`，他们都是从节点。

## 配置Hadoop的非安全模式

Hadoop的java配置通过两类非常重要的配置文件进行驱动：

* 只读的默认配置 - `core-default.xml`,`hdfs-default.xml`,`yarn-default.xml`以及`mapred-default.xml`。
* 个人的特殊配置 - `etc/hadoop/core-site.xml`,`etc/hadoop/hdfs-site.xml`,`yarn-site.xml`以及`mapred-site.xml`

另外，你可以在发行目录`bin`下找到你可以控制的脚本，通过`etc/hadoop/hadoop-env.sh`和`etc/hadoop/yarn-env.sh`配置一些个人特殊的值。

你需要配置Hadoop守护进程执行时的`environment`，以及为Hadoop守护进程设置配置参数。

HDFS守护进程有NameNode,SecondaryNameNode和DataNode。YARN守护进程有ResourceManager,NodeManager和WebAppProxy。如果使用了MapReduce，那么MapReduce Job History Server也要运行。对于庞大的安装情况，他们都会运行在不同的主机上。

### 配置Hadoop守护进程的环境

管理员应该使用`etc/hadoop/hadoop-env.sh`,`etc/hadoop/mapred-env.sh`,`etc/hadoop/yarn-env.sh`脚本自定义Hadoop守护进程的环境。

最后，你应该使`JAVA_HOME`在每一个远程节点上配置正确。

管理员可以使用下表中的配置选项配置个人的守护程序：

| 守护程序                      | 环境变量                      |
| ----------------------------- | ----------------------------- |
| NameNode                      | HADOOP_NAMENODE_OPTS          |
| DataNode                      | HADOOP_DATANODE_OPTS          |
| Secondary NameNode            | HADOOP_SECONDARYNAMENODE_OPTS |
| ResourceManager               | YARN_RESOURCEMANAGER_OPTS     |
| NodeManager                   | YARN_NODEMANAGER_OPTS         |
| WebAppProxy                   | YARN_PROXYSERVER_OPTS         |
| Map Reduce Job History Server | HADOOP_JOB_HISTORYSERVER_OPTS |

例如，配置Namenode使用`parallelGC`，下面的部分可以添加到`hadoop_env.sh`中：

```shell
export HADOOP_NAMENODE_OPTS="-XX:+UseParallelGC"
```

查看`etc/hadoop/hadoop-env.sh`查看其他的例子。

你可以配置的其他非常有用的参数包含：

* `HADOOP_PID_DIR` - 这个目录存储守护进程的id文件
* `HADOOP_LOG_DIR` - 这个目录存储守护进程的日志文件，日志文件会被自动的创建。
* `HADOOP_HEAPSIZE` / `YARN_HEAPSIZE` - 最大可用的堆容量，单位`MB`。例如：如果该变量被设置为1000，那么堆的大小会被设置为1000M。它被用于配置守护进程的堆大小。默认值为1000

大多数情况下，你应该指定`HADOOP_PID_DIR`和`HADOOP_LOG_DIR`，他们仅能被运行Hadoop守护进程的用户进行写入操作。否则可能会发生符号链接攻击。

也可以在整个shell的环境变量中配置`HADOOP_PREFIX`。例如，在`/etc/profile.d`中的一段简单的脚本：

```shell
HADOOP_PREFIX=/path/to/hadoop
export HADOOP_PREFIX
```

设置各个守护进程堆大小的环境变量：

| 守护进程                      | 环境变量                          |
| ----------------------------- | --------------------------------- |
| ResourceManager               | YARN_RESOURCEMANAGER_HEAPSIZE     |
| NodeManager                   | YARN_NODEMANAGER_HEAPSIZE         |
| WebAppProxy                   | YARN_PROXYSERVER_HEAPSIZE         |
| Map Reduce Job History Server | HADOOP_JOB_HISTORYSERVER_HEAPSIZE |

### 配置守护进程

这个章节中详解了给定配置文件的重要的参数：

* `etc/hadoop/core-site.xml`

| 参数                | 值            | 备注                     |
| ------------------- | ------------- | ------------------------ |
| fs.defaultFS        | NameNode的URL | hdfs://host:port/        |
| io.file.buffer.size | 131072        | 队列文件读写缓冲区的大小 |

* `etc/hadoop/hdfs-site.xml`
* NameNode配置：

| 参数                            | 值                                            | 备注                                                         |
| ------------------------------- | --------------------------------------------- | ------------------------------------------------------------ |
| dfs.namenode.name.dir           | NameNode存储namespace和事务日志的本地文件路径 | 如果多值为以逗号分隔的目录列表，那么名称表会冗余复制存储在所有的目录中 |
| dfs.hosts` / `dfs.hosts.exclude | 允许或排除的DataNode                          | 在有需要的情况下，通过文件来控制允许的数据节点               |
| dfs.blocksize                   | 268435456                                     | 在大型系统中，HDFS的块大小为256MB                            |
| dfs.namenode.handler.count      | 100                                           | 多个NameNode服务线程可以处理的DataNode的数量                 |

* DataNode配置

| 参数                  | 值                                               | 备注                                                         |
| --------------------- | ------------------------------------------------ | ------------------------------------------------------------ |
| dfs.datanode.data.dir | DataNode在本地系统中存储数据块的路径，以逗号分隔 | 如果是以逗号分隔的目录，那么数据会被存储在所有给定的目录中，通常会存储在不同的设备上 |

* `etc/hadoop/yarn-site.xml`
* ResourceManager与NodeManager的配置

| 参数                        | 值             | 备注                                                         |
| --------------------------- | -------------- | ------------------------------------------------------------ |
| yarn.acl.enable             | true` / `false | 是否支持ACL，默认为false                                     |
| yarn.admin.acl              | 管理员的ACL    | 在集群上设置管理员的ACL，默认值为*，意为所有人，如果指定为空格，就是所有人都不能访问 |
| yarn.log-aggregation-enable | false          | 配置开启或禁用日志聚合                                       |

* ResourceManager配置：

| Parameter                                                    | Value                                                        | Notes                                                        |
| :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| `yarn.resourcemanager.address`                               | `ResourceManager` host:port 为客户端提交工作的主机和端口     | 如果设置了*host:port*，则会覆盖掉在 `yarn.resourcemanager.hostname`中配置的主机名 |
| `yarn.resourcemanager.scheduler.address`                     | `ResourceManager` 为 ApplicationMasters 联系调度器获取字段的host:port | 如果设置了*host:port*，则会覆盖掉在 `yarn.resourcemanager.hostname`中配置的主机名 |
| `yarn.resourcemanager.resource-tracker.address`              | `ResourceManager` 提供给NodeManagers的host:port              | 如果设置了*host:port*，则会覆盖掉在 `yarn.resourcemanager.hostname`中配置的主机名 |
| `yarn.resourcemanager.admin.address`                         | `ResourceManager` 提供给管理员命令的 host:port               | 如果设置了*host:port*，则会覆盖掉在 `yarn.resourcemanager.hostname`中配置的主机名 |
| `yarn.resourcemanager.webapp.address`                        | `ResourceManager` web-ui host:port.                          | 如果设置了*host:port*，则会覆盖掉在 `yarn.resourcemanager.hostname`中配置的主机名 |
| `yarn.resourcemanager.hostname`                              | `ResourceManager` host.                                      | 主机可以设置一个主机名来替代所有`yarn.resourcemanager*address`的设置。 通过为ResourceManager提供默认的端口。 |
| `yarn.resourcemanager.scheduler.class`                       | `ResourceManager` 调度器类                                   | `CapacityScheduler` (推荐), `FairScheduler` (也推荐使用), or `FifoScheduler`. 使用全类名，例如：`org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler`. |
| `yarn.scheduler.minimum-allocation-mb`                       | `Resource Manager`上每一个容器的请求可以获取的内存的最小限制 | 单位：MB                                                     |
| `yarn.scheduler.maximum-allocation-mb`                       | `Resource Manager`上每一个容器的请求可以获取的内存的最大限制 | 单位：MB                                                     |
| `yarn.resourcemanager.nodes.include-path` / `yarn.resourcemanager.nodes.exclude-path` | 允许或排除的NodeManager列表                                  | 如果有必要的话，使用文件来控制允许的NodeManager的列表。      |

NodeManager配置

| Parameter                                    | Value                                                        | Notes                                                        |
| :------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| `yarn.nodemanager.resource.memory-mb`        | `NodeManager`可用的内存，单位：MB                            | `NodeManager`用于支持运行容器的总的可用资源                  |
| `yarn.nodemanager.vmem-pmem-ratio`           | 任务可以使用的虚拟内存的最大比率                             | 一个任务可以使用的虚拟内存占物理内存的比率，都受该参数的限制。NodeManager上的任务可以超出物理内存使用一共可以使用的虚拟内存的比率。 |
| `yarn.nodemanager.local-dirs`                | 本地系统中，数据可以写入的目录，以逗号分隔。                 | 多个路径有利于释放磁盘IO的压力                               |
| `yarn.nodemanager.log-dirs`                  | Comma-separated list of paths on the local filesystem where logs are written. | 多个路径有利于释放磁盘IO的压力                               |
| `yarn.nodemanager.log.retain-seconds`        | *10800*                                                      | NodeManager保留日志文件的时间，单位：秒，尽在日志聚合被禁用时有效。 |
| `yarn.nodemanager.remote-app-log-dir`        | */logs*                                                      | 在应用程序执行完成后，日志文件被移动的HDFS目录。需要设置相应的权限，尽在日志聚合被启用的时候生效。 |
| `yarn.nodemanager.remote-app-log-dir-suffix` | *logs*                                                       | 拼接到远程日志目录的后缀，日志会被聚合到${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam}，尽在日志聚合被启用的时候生效。 |
| `yarn.nodemanager.aux-services`              | mapreduce_shuffle                                            | 为MapReduce程序设置的Shuffle服务。                           |

* History Server的配置（需要被移动到别处）

| Parameter                                            | Value | Notes                                                        |
| :--------------------------------------------------- | :---- | :----------------------------------------------------------- |
| `yarn.log-aggregation.retain-seconds`                | *-1*  | 删除日志之前保持聚合的时间，-1为禁用，注意，设置的值太小，会导致名称节点收到过多消息，产生垃圾数据。 |
| `yarn.log-aggregation.retain-check-interval-seconds` | *-1*  | 两次日志聚合检查之间的间隔时长。如果设置为0或负值，该值会被计算为聚合日志保留时间的十分之一。设置的值太短会导致名称节点收到垃圾数据。 |

* `/etc/hadoop/mapred-site.xml`
* MapReduce应用程序配置

| Parameter                                 | Value     | Notes                                              |
| :---------------------------------------- | :-------- | :------------------------------------------------- |
| `mapreduce.framework.name`                | yarn      | 执行框架设置为 Hadoop YARN.                        |
| `mapreduce.map.memory.mb`                 | 1536      | map使用的最大资源限制                              |
| `mapreduce.map.java.opts`                 | -Xmx1024M | map中子JVM使用的最大堆容量                         |
| `mapreduce.reduce.memory.mb`              | 3072      | reduce使用的最大资源限制                           |
| `mapreduce.reduce.java.opts`              | -Xmx2560M | reduce中子JVM堆的最大内存                          |
| `mapreduce.task.io.sort.mb`               | 512       | 数据排序时可以使用的自大内存限制，会影响到排序效率 |
| `mapreduce.task.io.sort.factor`           | 100       | 排序文件时，每次可合并数据流的数量                 |
| `mapreduce.reduce.shuffle.parallelcopies` | 50        | reduce获取批量map产生的输出时可以并行拷贝的数量。  |

* MapReduce JobHistory Server 配置

| Parameter                                    | Value                                          | Notes                                               |
| :------------------------------------------- | :--------------------------------------------- | :-------------------------------------------------- |
| `mapreduce.jobhistory.address`               | MapReduce JobHistory Server *host:port*        | 默认是10020.                                        |
| `mapreduce.jobhistory.webapp.address`        | MapReduce JobHistory Server Web UI *host:port* | 默认端口为19888.                                    |
| `mapreduce.jobhistory.intermediate-done-dir` | /mr-history/tmp                                | Mapreduce工作写出历史文件的目录。                   |
| `mapreduce.jobhistory.done-dir`              | /mr-history/done                               | 所有被MapReduce JobHistory Server管理的历史文件目录 |

## 监控NodeManager的健康情况

Hadoop提供了一种管理员可以配置NodeManager运行一个定期脚本去检查节点是否健康的机制。

管理员可以通过在脚本中执行对其选择的任何检查来确定节点是否处于正常状态。如果脚本检测到节点处于正常的状态，必须打印一个由ERROR开头的标准字符串。NodeManager定义产生检查输出的脚本。如果脚本输出包含了ERROR字符串，就会像上面描述的那样，节点的状态会被作为`unhealthy`进行报告，并将该节点加入到NodeManager的黑名单列表中。将来的任务也不会分配到该节点。NodeManager会持续的运行脚本，如果该节点变为正常状态，NodeManager会自动的将该节点从黑名单列表中移除。节点的状态完全依赖脚本的输出，它支持ResourceManager web界面。节点的状态会展示在web界面上。

下面的参数可以用于控制节点的健康监听器，它们位于`/etc/hadoop/yarn-site.xml`.

| Parameter                                           | Value                      | Notes                                     |
| :-------------------------------------------------- | :------------------------- | :---------------------------------------- |
| `yarn.nodemanager.health-checker.script.path`       | 节点的健康检查脚本的路径   | Script to check for node’s health status. |
| `yarn.nodemanager.health-checker.script.opts`       | 节点健康检查脚本的参数     | 为健康检查脚本提供的参数                  |
| `yarn.nodemanager.health-checker.interval-ms`       | 执行健康检查脚本的间隔时间 | 运行健康检查脚本的间隔时间                |
| `yarn.nodemanager.health-checker.script.timeout-ms` | 健康检查脚本的超时时间     | 执行健康检查脚本超时的时间                |

健康检查脚本不应该仅仅由于本地磁盘出现文件而给出ERROR，NodeManager有能力去定期检查本地磁盘的状态，并基于配置在yarn.nodemanager.disk-health-checker.min-healthy-disks配置的值，收集目录错误的数量是否达到阀，整个节点就会被标注为不正常被将此信息发送给资源管理器。

## 从文件

`/etc/hadoop/slaves`文件的每一行列出了所有的从节点的主机名或IP地址。帮助会使用`etc/hadoop/slaves`文件同时在多个主机上执行。它不会被任何基于Java的Hadoop配置文件使用，使用该功能，运行Hadoop的账户必须建立ssh信任链接。

## Hadoop机架感知

多个Hadoop组件之间可以通过网络拓扑结构实现机架感知，从而实现更高的安全性和更好的性能。Hadoop守护进程可以通过管理员配置的模块获得集群中从节点的机架信息。可以通过查看**Rack Awareness**查看更多信息。

非常推荐在启动HDFS之前配置机架感知。

## 日志

Hadoop使用**Apache log4j**，通过Apache Commons Logging框架打印日志。修改`/etc/hadoop/log4j.properties`文件可自定义Hadoop守护进程的日志配置。

## 操作Hadoop集群

完成每个必要的配置，将文件分发到所有主机的`HADOOP_CONF_DIR`目录，必须在所有的主机上都是相同的目录。

通常，推荐HDFS和YARN使用不同的账户运行，多数情况下，HDFS进程使用hdfs账户执行，YARN使用yarn账户执行。

### 启动Hadoop

启动Hadoop集群时需要同时启动HDFS和YARN集群。

启动HDFS的第一步，必须先进行格式化。格式化新的分布式文件系统为hdfs:

```shell
[hdfs]$ $HADOOP_PREFIX/bin/hdfs namenode -format <cluster_name>
```

使用下面的命令启动HDFS名称节点，并指定hdfs:

```shell
[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
```

启动HDFS数据节点，并指定hdfs：

```shell
[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanode
```

如果`/etc/hadoop/slaves`配置了ssh，所有的HDFS进程可以使用下面的脚本进行启动：

```shell
[hdfs]$ $HADDOP_PREFIX/sbin/start-dfs.sh
```

通过下面的命令启动yarn上指定的ResourceManager：

```shell
[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager
```

运行脚本启动每个主机上的NodeManager：

```shell
[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR start nodemanager
```

启动独立的WebAppProxy,多个服务负载均衡需在每个服务器上运行它

```shell
[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR start proxyserver
```

启动yarn

```shell
[yarn]$ $HADOOP_PREFIX/sbin/start-yarn.sh
```

启动MapReduce JobHistory Server

```shell
[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistopry-daemon.sh --config $HADOOP_CONF_DIR start historyserver
```

## 停止Hadoop

停止NameNode

```shell
[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode
```

停止DataNode

```shell
[hdfs]$ $HADOOP_PREFIX/sbin/hadoop_daemons.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode
```

停止hdfs

```shell
[hdfs]$ $HADOOP_PREFIX/sbin/stop-dfs.sh
```

停止ResourceManager

```shell
[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager
```

停止NodeManager

```shell
[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop nodemanager
```

停止yarn

```shell
[yarn]$ $HADOOP_YARN_HOME/sbin/stop-yarn.sh
```

停止WebAppProxy

```shell
[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop proxyserver
```

停止MapReduce History Server

```shell
[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR stop historyserver
```

## Web界面

Hadoop启动后可以通过web页面进行检查

| Daemon                      | Web Interface         | Notes                 |
| :-------------------------- | :-------------------- | :-------------------- |
| NameNode                    | http://nn_host:port/  | 默认http端口是 50070. |
| ResourceManager             | http://rm_host:port/  | 默认http端口是 8088.  |
| MapReduce JobHistory Server | http://jhs_host:port/ | 默认http端口是 19888. |